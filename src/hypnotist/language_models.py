import pickle
import openai
import os
from typing import Dict, List
import time


class Cache:
    def __init__(self, model_version) -> None:
        path_compatible_model_version = model_version.replace("/", "-")
        self.cache_file_name = f"cache_{path_compatible_model_version}.pkl"
        try:
            self.cache = pickle.load(open(self.cache_file_name, "rb"))
        except FileNotFoundError:
            self.cache = {}

    def get(self, key):
        return self.cache.get(key)

    def set(self, key, value):
        self.cache[key] = value

    def save(self):
        pickle.dump(self.cache, open(self.cache_file_name, "wb"))


class ChaoGPT:
    API_RETRY_SLEEP = 10
    API_ERROR_OUTPUT = "$ERROR$"
    API_QUERY_SLEEP = 0.5
    API_MAX_RETRY = 5
    # request timeout is no longer a param for openai > 1.0.0
    API_TIMEOUT = 20

    def __init__(self, model_name) -> None:
        self.model_name = model_name
        self.client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    def generate(
        self, conv: List[Dict], max_n_tokens: int, temperature: float, top_p: float
    ):
        """
        Args:
            conv: List of dictionaries, OpenAI API format
            max_n_tokens: int, max number of tokens to generate
            temperature: float, temperature for sampling
            top_p: float, top p for sampling
        Returns:
            str: generated response
        """
        output = self.API_ERROR_OUTPUT
        for _ in range(self.API_MAX_RETRY):
            try:
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=conv,
                    max_tokens=max_n_tokens,
                    temperature=temperature,
                    top_p=top_p,
                )
                output = response.choices[0].message.content
                break
            except openai.OpenAIError as e:
                print(type(e), e)
                time.sleep(self.API_RETRY_SLEEP)

            time.sleep(self.API_QUERY_SLEEP)
        return output

    def batched_generate(
        self,
        convs_list: List[List[Dict]],
        max_n_tokens: int,
        temperature: float,
        top_p: float = 1.0,
    ):
        return [
            self.generate(conv, max_n_tokens, temperature, top_p) for conv in convs_list
        ]
